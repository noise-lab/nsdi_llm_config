@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{telemetry,
author = {Gong, Fengchen and Raghunathan, Divya and Gupta, Aarti and Apostolaki, Maria},
title = {Zoom2Net: Constrained Network Telemetry Imputation},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672225},
doi = {10.1145/3651890.3672225},
abstract = {Fine-grained monitoring is crucial for multiple data-driven tasks such as debugging, provisioning, and securing networks. Yet, practical constraints in collecting, extracting, and storing data often force operators to use coarse-grained sampled monitoring, degrading the performance of the various tasks. In this work, we explore the feasibility of leveraging the correlations among coarse-grained time series to impute their fine-grained counterparts in software. We present Zoom2Net, a transformer-based model for network imputation that incorporates domain knowledge through operational and measurement constraints, ensuring that the imputed network telemetry time series are not only realistic but align with existing measurements. This approach enhances the capabilities of current monitoring infrastructures, allowing operators to gain more insights into system behaviors without the need for hardware upgrades. We evaluate Zoom2Net on four diverse datasets (e.g., cloud telemetry and Internet data transfer) and use cases (e.g., bursts analysis and traffic classification). We demonstrate that Zoom2Net consistently achieves high imputation accuracy with a zoom-in factor of up to 100 and performs better on downstream tasks compared to baselines by an average of 38\%.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {764–777},
numpages = {14},
keywords = {telemetry, imputation, formal methods, transformer},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}

@misc{wu2024netllm,
      title={NetLLM: Adapting Large Language Models for Networking}, 
      author={Duo Wu and Xianda Wang and Yaqi Qiao and Zhi Wang and Junchen Jiang and Shuguang Cui and Fangxin Wang},
      year={2024},
      eprint={2402.02338},
      archivePrefix={arXiv},
      primaryClass={cs.NI}
}

@inproceedings{10.1145/3563766.3564104,
author = {Dietm\"{u}ller, Alexander and Ray, Siddhant and Jacob, Romain and Vanbever, Laurent},
title = {A new hope for network model generalization},
year = {2022},
isbn = {9781450398992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563766.3564104},
doi = {10.1145/3563766.3564104},
abstract = {Generalizing machine learning (ML) models for network traffic dynamics tends to be considered a lost cause. Hence for every new task, we design new models and train them on model-specific datasets closely mimicking the deployment environments. Yet, an ML architecture called Transformer has enabled previously unimaginable generalization in other domains. Nowadays, one can download a model pre-trained on massive datasets and only fine-tune it for a specific task and context with comparatively little time and data. These fine-tuned models are now state-of-the-art for many benchmarks.We believe this progress could translate to networking and propose a Network Traffic Transformer (NTT), a transformer adapted to learn network dynamics from packet traces. Our initial results are promising: NTT seems able to generalize to new prediction tasks and environments. This study suggests there is still hope for generalization through future research.},
booktitle = {Proceedings of the 21st ACM Workshop on Hot Topics in Networks},
pages = {152–159},
numpages = {8},
keywords = {transformer, packet-level modeling},
location = {Austin, Texas},
series = {HotNets '22}
}


@misc{nie2023time,
      title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers}, 
      author={Yuqi Nie and Nam H. Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
      year={2023},
      eprint={2211.14730},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2021informer,
      title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}, 
      author={Haoyi Zhou and Shanghang Zhang and Jieqi Peng and Shuai Zhang and Jianxin Li and Hui Xiong and Wancai Zhang},
      year={2021},
      eprint={2012.07436},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.07436}, 
}

@misc{wu2022autoformer,
      title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}, 
      author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
      year={2022},
      eprint={2106.13008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.13008}, 
}

@misc{zhou2022fedformer,
      title={FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting}, 
      author={Tian Zhou and Ziqing Ma and Qingsong Wen and Xue Wang and Liang Sun and Rong Jin},
      year={2022},
      eprint={2201.12740},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.12740}, 
}



@misc{guthula2023netfound,
      title={netFound: Foundation Model for Network Security}, 
      author={Satyandra Guthula and Navya Battula and Roman Beltiukov and Wenbo Guo and Arpit Gupta},
      year={2023},
      eprint={2310.17025},
      archivePrefix={arXiv},
      primaryClass={cs.NI}
}

@article{Gehring2017ConvolutionalST,
  title={Convolutional Sequence to Sequence Learning},
  author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann Dauphin},
  journal={ArXiv},
  year={2017},
  volume={abs/1705.03122},
  url={https://api.semanticscholar.org/CorpusID:3648736}
}

@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{li2019enhancing,
  title={Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting},
  author={Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{lim2021temporal,
  title={Temporal fusion transformers for interpretable multi-horizon time series forecasting},
  author={Lim, Bryan and Ar{\i}k, Sercan {\"O} and Loeff, Nicolas and Pfister, Tomas},
  journal={International Journal of Forecasting},
  volume={37},
  number={4},
  pages={1748--1764},
  year={2021},
  publisher={Elsevier}
}

@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}

@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  number={9},
  pages={11121--11128},
  year={2023}
}

@article{jiang2023ac,
  title={AC-DC: Adaptive Ensemble Classification for Network Traffic Identification},
  author={Jiang, Xi and Liu, Shinan and Naama, Saloua and Bronzino, Francesco and Schmitt, Paul and Feamster, Nick},
  journal={arXiv preprint arXiv:2302.11718},
  year={2023}
}
@inproceedings{jiang2023generative,
  title={Generative, high-fidelity network traces},
  author={Jiang, Xi and Liu, Shinan and Gember-Jacobson, Aaron and Schmitt, Paul and Bronzino, Francesco and Feamster, Nick},
  booktitle={Proceedings of the 22nd ACM Workshop on Hot Topics in Networks},
  pages={131--138},
  year={2023}
}
@inproceedings{brown2023augmenting,
  title={Augmenting rule-based dns censorship detection at scale with machine learning},
  author={Brown, Jacob and Jiang, Xi and Tran, Van and Bhagoji, Arjun Nitin and Hoang, Nguyen Phong and Feamster, Nick and Mittal, Prateek and Yegneswaran, Vinod},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3750--3761},
  year={2023}
}

@article{jiang2024netdiffusion,
  title={NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation},
  author={Jiang, Xi and Liu, Shinan and Gember-Jacobson, Aaron and Bhagoji, Arjun Nitin and Schmitt, Paul and Bronzino, Francesco and Feamster, Nick},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={8},
  number={1},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY, USA}
}
@article{sivaroopan2023netdiffus,
  title={Netdiffus: Network traffic generation by diffusion models through time-series imaging},
  author={Sivaroopan, Nirhoshan and Bandara, Dumindu and Madarasingha, Chamara and Jourjon, Guilluame and Jayasumana, Anura and Thilakarathna, Kanchana},
  journal={arXiv preprint arXiv:2310.04429},
  year={2023}
}


@article{jin2020swiftids,
  title={SwiftIDS: Real-time intrusion detection system based on LightGBM and parallel intrusion detection mechanism},
  author={Jin, Dongzi and Lu, Yiqin and Qin, Jiancheng and Cheng, Zhe and Mao, Zhongshu},
  journal={Computers \& Security},
  volume={97},
  pages={101984},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{lai2020using,
  title={Using xgboost for cyberattack detection and analysis in a network log system with elk stack},
  author={Lai, Cing-Han and Yang, Chao-Tung and Kristiani, Endah and Liu, Jung-Chun and Chan, Yu-Wei},
  booktitle={Frontier Computing: Theory, Technologies and Applications (FC 2019) 8},
  pages={302--311},
  year={2020},
  organization={Springer}
}

@inproceedings{akem2023flowrest,
  title={Flowrest: Practical Flow-Level Inference in Programmable Switches with Random Forests},
  author={Akem, Aristide Tanyi-Jong and Gucciardo, Michele and Fiore, Marco and others},
  booktitle={IEEE International Conference on Computer Communications},
  year={2023}
}


@article{lotfollahi2020deep,
  title={Deep packet: A novel approach for encrypted traffic classification using deep learning},
  author={Lotfollahi, Mohammad and Jafari Siavoshani, Mahdi and Shirali Hossein Zade, Ramin and Saberian, Mohammdsadegh},
  journal={Soft Computing},
  volume={24},
  number={3},
  pages={1999--2012},
  year={2020},
  publisher={Springer}
}

@article{kim2018web,
  title={Web traffic anomaly detection using C-LSTM neural networks},
  author={Kim, Tae-Young and Cho, Sung-Bae},
  journal={Expert Systems with Applications},
  volume={106},
  pages={66--76},
  year={2018},
  publisher={Elsevier}
}

@article{radford2018network,
  title={Network traffic anomaly detection using recurrent neural networks},
  author={Radford, Benjamin J and Apolonio, Leonardo M and Trias, Antonio J and Simpson, Jim A},
  journal={arXiv preprint arXiv:1803.10769},
  year={2018}
}
@article{liu2023real,
  title={Real-Time Anomaly Detection of Network Traffic Based on CNN},
  author={Liu, Haitao and Wang, Haifeng},
  journal={Symmetry},
  volume={15},
  number={6},
  pages={1205},
  year={2023},
  publisher={MDPI}
}

@inproceedings{aouedi2021performance,
  title={Performance evaluation of feature selection and tree-based algorithms for traffic classification},
  author={Aouedi, Ons and Piamrat, Kandaraj and Parrein, Beno{\^\i}t},
  booktitle={2021 IEEE International Conference on Communications Workshops (ICC Workshops)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}


@inproceedings{ismailaj2021deep,
  title={When deep learning may not be the right tool for traffic classification},
  author={Ismailaj, Kleidi and Camelo, Miguel and Latr{\'e}, Steven},
  booktitle={2021 IFIP/IEEE International Symposium on Integrated Network Management (IM)},
  pages={884--889},
  year={2021},
  organization={IEEE}
}

@Inbook{ns3,
author="Riley, George F.
and Henderson, Thomas R.",
editor="Wehrle, Klaus
and G{\"u}ne{\c{s}}, Mesut
and Gross, James",
title="The ns-3 Network Simulator",
bookTitle="Modeling and Tools for Network Simulation",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="15--34",
abstract="As networks of computing devices grow larger and more complex, the need for highly accurate and scalable network simulation technologies becomes critical. Despite the emergence of large-scale testbeds for network research, simulation still plays a vital role in terms of scalability (both in size and in experimental speed), reproducibility, rapid prototyping, and education. With simulation based studies, the approach can be studied in detail at varying scales, with varying data applications, varying field conditions, and will result in reproducible and analyzable results.",
isbn="978-3-642-12331-3",
doi="10.1007/978-3-642-12331-3_2",
url="https://doi.org/10.1007/978-3-642-12331-3_2"
}


@inproceedings{10.1145/3230543.3230564,
author = {Montazeri, Behnam and Li, Yilong and Alizadeh, Mohammad and Ousterhout, John},
title = {Homa: a receiver-driven low-latency transport protocol using network priorities},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230564},
doi = {10.1145/3230543.3230564},
abstract = {Homa is a new transport protocol for datacenter networks. It provides exceptionally low latency, especially for workloads with a high volume of very short messages, and it also supports large messages and high network utilization. Homa uses in-network priority queues to ensure low latency for short messages; priority allocation is managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Homa also uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high load. Our implementation of Homa delivers 99th percentile round-trip times less than 15 μs for short messages on a 10 Gbps network running at 80\% load. These latencies are almost 100x lower than the best published measurements of an implementation. In simulations, Homa's latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {221–235},
numpages = {15},
keywords = {data centers, low latency, network stacks, transport protocols},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@article{10.1145/1851275.1851192,
author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
title = {Data center TCP (DCTCP)},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/1851275.1851192},
doi = {10.1145/1851275.1851192},
abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic.To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {63–74},
numpages = {12},
keywords = {ECN, TCP, data center network}
}

@inproceedings{10.1145/1851182.1851192,
author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
title = {Data center TCP (DCTCP)},
year = {2010},
isbn = {9781450302012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1851182.1851192},
doi = {10.1145/1851182.1851192},
abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic.To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
booktitle = {Proceedings of the ACM SIGCOMM 2010 Conference},
pages = {63–74},
numpages = {12},
keywords = {ECN, TCP, data center network},
location = {New Delhi, India},
series = {SIGCOMM '10}
}

@inproceedings{10.1145/2785956.2787472,
author = {Roy, Arjun and Zeng, Hongyi and Bagga, Jasmeet and Porter, George and Snoeren, Alex C.},
title = {Inside the Social Network's (Datacenter) Network},
year = {2015},
isbn = {9781450335423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2785956.2787472},
doi = {10.1145/2785956.2787472},
abstract = {Large cloud service providers have invested in increasingly larger datacenters to house the computing infrastructure required to support their services. Accordingly, researchers and industry practitioners alike have focused a great deal of effort designing network fabrics to efficiently interconnect and manage the traffic within these datacenters in performant yet efficient fashions. Unfortunately, datacenter operators are generally reticent to share the actual requirements of their applications, making it challenging to evaluate the practicality of any particular design.Moreover, the limited large-scale workload information available in the literature has, for better or worse, heretofore largely been provided by a single datacenter operator whose use cases may not be widespread. In this work, we report upon the network traffic observed in some of Facebook's datacenters. While Facebook operates a number of traditional datacenter services like Hadoop, its core Web service and supporting cache infrastructure exhibit a number of behaviors that contrast with those reported in the literature. We report on the contrasting locality, stability, and predictability of network traffic in Facebook's datacenters, and comment on their implications for network architecture, traffic engineering, and switch design.},
booktitle = {Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication},
pages = {123–137},
numpages = {15},
keywords = {datacenter traffic patterns},
location = {London, United Kingdom},
series = {SIGCOMM '15}
}

@article{10.1145/2829988.2787472,
author = {Roy, Arjun and Zeng, Hongyi and Bagga, Jasmeet and Porter, George and Snoeren, Alex C.},
title = {Inside the Social Network's (Datacenter) Network},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2829988.2787472},
doi = {10.1145/2829988.2787472},
abstract = {Large cloud service providers have invested in increasingly larger datacenters to house the computing infrastructure required to support their services. Accordingly, researchers and industry practitioners alike have focused a great deal of effort designing network fabrics to efficiently interconnect and manage the traffic within these datacenters in performant yet efficient fashions. Unfortunately, datacenter operators are generally reticent to share the actual requirements of their applications, making it challenging to evaluate the practicality of any particular design.Moreover, the limited large-scale workload information available in the literature has, for better or worse, heretofore largely been provided by a single datacenter operator whose use cases may not be widespread. In this work, we report upon the network traffic observed in some of Facebook's datacenters. While Facebook operates a number of traditional datacenter services like Hadoop, its core Web service and supporting cache infrastructure exhibit a number of behaviors that contrast with those reported in the literature. We report on the contrasting locality, stability, and predictability of network traffic in Facebook's datacenters, and comment on their implications for network architecture, traffic engineering, and switch design.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {123–137},
numpages = {15},
keywords = {datacenter traffic patterns}
}

@misc{bang2023vtrain,
      title={vTrain: A Simulation Framework for Evaluating Cost-effective and Compute-optimal Large Language Model Training}, 
      author={Jehyeon Bang and Yujeong Choi and Myeongwoo Kim and Yongdeok Kim and Minsoo Rhu},
      year={2023},
      eprint={2312.12391},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{red,
  author={Floyd, S. and Jacobson, V.},
  journal={IEEE/ACM Transactions on Networking}, 
  title={Random early detection gateways for congestion avoidance}, 
  year={1993},
  volume={1},
  number={4},
  pages={397-413},
  keywords={Transport protocols;Delay effects;Throughput;Propagation delay;Bandwidth;TCPIP;Traffic control;High-speed networks;Feedback;Delay estimation},
  doi={10.1109/90.251892}}

@article{codel,
author = {Nichols, Kathleen and Jacobson, Van},
title = {Controlling queue delay},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/2209249.2209264},
doi = {10.1145/2209249.2209264},
abstract = {A modern AQM is just one piece of the solution to bufferbloat.},
journal = {Commun. ACM},
month = {jul},
pages = {42–50},
numpages = {9}
}

@misc{fqcodel,
    series =    {Request for Comments},
    number =    8290,
    howpublished =  {RFC 8290},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC8290},
    url =       {https://www.rfc-editor.org/info/rfc8290},
    author =    {Toke Høiland-Jørgensen and Paul McKenney and dave.taht@gmail.com and Jim Gettys and Eric Dumazet},
    title =     {{The Flow Queue CoDel Packet Scheduler and Active Queue Management Algorithm}},
    pagetotal = 25,
    year =      2018,
    month =     jan,
    abstract =  {This memo presents the FQ-CoDel hybrid packet scheduler and Active Queue Management (AQM) algorithm, a powerful tool for fighting bufferbloat and reducing latency. FQ-CoDel mixes packets from multiple flows and reduces the impact of head-of-line blocking from bursty traffic. It provides isolation for low-rate traffic such as DNS, web, and videoconferencing traffic. It improves utilisation across the networking fabric, especially for bidirectional traffic, by keeping queue lengths short, and it can be implemented in a memory- and CPU-efficient fashion across a wide range of hardware.},
}

@article{bufferbloat,
author = {Gettys, Jim and Nichols, Kathleen},
title = {Bufferbloat: Dark Buffers in the Internet: Networks without effective AQM may again be vulnerable to congestion collapse.},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {11},
issn = {1542-7730},
url = {https://doi.org/10.1145/2063166.2071893},
doi = {10.1145/2063166.2071893},
abstract = {Today’s networks are suffering from unnecessary latency and poor system performance. The culprit is bufferbloat, the existence of excessively large and frequently full buffers inside the network. Large buffers have been inserted all over the Internet without sufficient thought or testing. They damage or defeat the fundamental congestion-avoidance algorithms of the Internet’s most common transport protocol. Long delays from bufferbloat are frequently attributed incorrectly to network congestion, and this misinterpretation of the problem leads to the wrong solutions being proposed.},
journal = {Queue},
month = {nov},
pages = {40–54},
numpages = {15}
}

@misc{l4s,
      title={Dual Queue Coupled AQM: Deployable Very Low Queuing Delay for All}, 
      author={Koen De Schepper and Olga Albisser and Olivier Tilmans and Bob Briscoe},
      year={2022},
      eprint={2209.01078},
      archivePrefix={arXiv},
      primaryClass={cs.NI},
      url={https://arxiv.org/abs/2209.01078}, 
}

@misc{l4s2,
    series =    {Request for Comments},
    number =    9330,
    howpublished =  {RFC 9330},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC9330},
    url =       {https://www.rfc-editor.org/info/rfc9330},
    author =    {Bob Briscoe and Koen De Schepper and Marcelo Bagnulo and Greg White},
    title =     {{Low Latency, Low Loss, and Scalable Throughput (L4S) Internet Service: Architecture}},
    pagetotal = 36,
    year =      2023,
    month =     jan,
    abstract =  {This document describes the L4S architecture, which enables Internet applications to achieve low queuing latency, low congestion loss, and scalable throughput control. L4S is based on the insight that the root cause of queuing delay is in the capacity-seeking congestion controllers of senders, not in the queue itself. With the L4S architecture, all Internet applications could (but do not have to) transition away from congestion control algorithms that cause substantial queuing delay and instead adopt a new class of congestion controls that can seek capacity with very little queuing. These are aided by a modified form of Explicit Congestion Notification (ECN) from the network. With this new architecture, applications can have both low latency and high throughput. The architecture primarily concerns incremental deployment. It defines mechanisms that allow the new class of L4S congestion controls to coexist with 'Classic' congestion controls in a shared network. The aim is for L4S latency and throughput to be usually much better (and rarely worse) while typically not impacting Classic performance.},
}

@misc{l4s3,
    series =    {Request for Comments},
    number =    9331,
    howpublished =  {RFC 9331},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC9331},
    url =       {https://www.rfc-editor.org/info/rfc9331},
    author =    {Koen De Schepper and Bob Briscoe},
    title =     {{The Explicit Congestion Notification (ECN) Protocol for Low Latency, Low Loss, and Scalable Throughput (L4S)}},
    pagetotal = 52,
    year =      2023,
    month =     jan,
    abstract =  {This specification defines the protocol to be used for a new network service called Low Latency, Low Loss, and Scalable throughput (L4S). L4S uses an Explicit Congestion Notification (ECN) scheme at the IP layer that is similar to the original (or 'Classic') ECN approach, except as specified within. L4S uses 'Scalable' congestion control, which induces much more frequent control signals from the network, and it responds to them with much more fine-grained adjustments so that very low (typically sub-millisecond on average) and consistently low queuing delay becomes possible for L4S traffic without compromising link utilization. Thus, even capacity-seeking (TCP-like) traffic can have high bandwidth and very low delay at the same time, even during periods of high traffic load. The L4S identifier defined in this document distinguishes L4S from 'Classic' (e.g., TCP-Reno-friendly) traffic. Then, network bottlenecks can be incrementally modified to distinguish and isolate existing traffic that still follows the Classic behaviour, to prevent it from degrading the low queuing delay and low loss of L4S traffic. This Experimental specification defines the rules that L4S transports and network elements need to follow, with the intention that L4S flows neither harm each other's performance nor that of Classic traffic. It also suggests open questions to be investigated during experimentation. Examples of new Active Queue Management (AQM) marking algorithms and new transports (whether TCP-like or real time) are specified separately.},
}

@misc{ecn,
    series =    {Request for Comments},
    number =    8087,
    howpublished =  {RFC 8087},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC8087},
    url =       {https://www.rfc-editor.org/info/rfc8087},
    author =    {Gorry Fairhurst and Michael Welzl},
    title =     {{The Benefits of Using Explicit Congestion Notification (ECN)}},
    pagetotal = 19,
    year =      2017,
    month =     mar,
    abstract =  {The goal of this document is to describe the potential benefits of applications using a transport that enables Explicit Congestion Notification (ECN). The document outlines the principal gains in terms of increased throughput, reduced delay, and other benefits when ECN is used over a network path that includes equipment that supports Congestion Experienced (CE) marking. It also discusses challenges for successful deployment of ECN. It does not propose new algorithms to use ECN nor does it describe the details of implementation of ECN in endpoint devices (Internet hosts), routers, or other network devices.},
}


@techreport{l4s_deployment_1,
    author = {Mitchell Clark},
    title = {The quiet plan to make the internet feel faster},
    institution = {The Verge},
    url = {https://www.theverge.com/23655762/l4s-internet-apple-comcast-latency-speed-bandwidth}, 
    year = {2023},
}

@techreport{l4s_deployment_2,
    number =    {draft-livingood-low-latency-deployment-05},
    type =      {Internet-Draft},
    institution =   {Internet Engineering Task Force},
    publisher = {Internet Engineering Task Force},
    note =      {Work in Progress},
    url =       {https://datatracker.ietf.org/doc/draft-livingood-low-latency-deployment/05/},
    author =    {Jason Livingood},
    title =     {{ISP Dual Queue Networking Deployment Recommendations}},
    pagetotal = 15,
    year =      2024,
    month =     apr,
    day =       18,
    abstract =  {The IETF's Transport Area Working Group (TSVWG) has finalized experimental RFCs for Low Latency, Low Loss, Scalable Throughput (L4S) and new Non-Queue-Building (NQB) per hop behavior. These documents do a good job of describing a new architecture and protocol for deploying low latency networking. But as is normal for many such standards, especially those in experimental status, certain deployment decisions are ultimately left to implementers. This document explores the potential implications of key deployment decisions and makes recommendations for those decisions that may help drive adoption.},
}

@techreport{l4s_deployment_3,
    author = {Nokia Bell Labs},
    title = {L4S},
    institution = {Nokia Bell Labs},
    url = {https://www.bell-labs.com/research-innovation/projects-and-initiatives/l4s/#gref},
    year = {2023},
}

@techreport{l4s_deployment_4,
    author = {Yanitsa Boyadzhieva},
    title = {Verizon targets next wave of time-critical 5G apps with L4S},
    institution = {TelecomTV},
    url = {https://www.telecomtv.com/content/5g/verizon-targets-next-wave-of-time-critical-5g-apps-with-l4s-49601/},
    year = {2024},
}

@INPROCEEDINGS{bilstm,
  author={Siami-Namini, Sima and Tavakoli, Neda and Namin, Akbar Siami},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={The Performance of LSTM and BiLSTM in Forecasting Time Series}, 
  year={2019},
  volume={},
  number={},
  pages={3285-3292},
  keywords={Biological system modeling;Training;Data models;Logic gates;Time series analysis;Predictive models;Recurrent neural networks},
  doi={10.1109/BigData47090.2019.9005997}}

@inproceedings{datacenter_traces,
author = {Woodruff, Jackson and Moore, Andrew W and Zilberman, Noa},
title = {Measuring Burstiness in Data Center Applications},
year = {2020},
isbn = {9781450377454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375235.3375240},
doi = {10.1145/3375235.3375240},
abstract = {Buffer sizing is a tricky task --- it depends on a large number of variables, ranging from congestion control to traffic engineering. Still, the most unpredictable contributors are the workloads running in the network. The link utilization and burstiness of these workloads dictate the buffer depth needed by a switch. But what is a burst? Do traditional definitions still apply in the age in which switches transfer terabits of data and billions of packets every second? Unless we assess bursts correctly, we are unlikely to size buffers appropriately. In this work, we present a measurement-led evaluation of the burstiness of different data center applications. We address the question of "what is a burst?" and assert that common techniques cannot answer this question in modern data centers. We quantify the change in burstiness of the studied applications across multiple vectors, including latency and network perspective, and generalize our results to the common case. Our observations can inform future buffer sizing efforts and guide switch configurations. Our dataset is openly available for the benefit of the community.},
booktitle = {Proceedings of the 2019 Workshop on Buffer Sizing},
articleno = {5},
numpages = {6},
location = {Palo Alto, CA, USA},
series = {BS '19}
}

@book{ewma,
  added-at = {2009-08-21T10:31:17.000+0200},
  address = {New York, NY [u.a.]},
  author = {Montgomery, {Douglas C.}},
  biburl = {https://www.bibsonomy.org/bibtex/28baae93ab2e0a4f5f1e68f35045dddb5/fbw_hannover},
  edition = {3. ed},
  interhash = {9ace6627e213aa4cb27d903b7b50caa5},
  intrahash = {8baae93ab2e0a4f5f1e68f35045dddb5},
  isbn = {0471303534},
  keywords = {Process_control Quality_control Qualitätssicherung Statistical_methods Statistische_Qualitätskontrolle},
  pagetotal = {XIX, 677, [44]},
  ppn_gvk = {192972065},
  publisher = {Wiley},
  timestamp = {2009-08-21T10:32:02.000+0200},
  title = {Introduction to statistical quality control},
  url = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+192972065&sourceid=fbw_bibsonomy},
  year = 1997
}


@article{arima,
	article_type = {journal},
	author = {Valipour, Mohammad and Banihabib, Mohammad Ebrahim and Behbahani, Seyyed Mahmood Reza},
	doi = {10.3844/jmssp.2012.330.338},
	journal = {Journal of Mathematics and Statistics},
	month = {Aug},
	pages = {330-338},
	publisher = {Science Publications},
	title = {Parameters Estimate of Autoregressive Moving Average and Autoregressive Integrated Moving Average Models and Compare Their Ability for Inflow Forecasting},
	url = {https://thescipub.com/abstract/jmssp.2012.330.338},
	volume = {8},
	year = {2012},
	bdsk-url-1 = {https://thescipub.com/abstract/jmssp.2012.330.338},
	bdsk-url-2 = {https://doi.org/10.3844/jmssp.2012.330.338}}

@inproceedings{m3,
author = {Li, Chenning and Nasr-Esfahany, Arash and Zhao, Kevin and Noorbakhsh, Kimia and Goyal, Prateesh and Alizadeh, Mohammad and Anderson, Thomas E.},
title = {m3: Accurate Flow-Level Performance Estimation using Machine Learning},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651890.3672243},
doi = {10.1145/3651890.3672243},
abstract = {Data center network operators often need accurate estimates of aggregate network performance. Unfortunately, existing methods for estimating aggregate network statistics are either inaccurate or too slow to be practical at the data center scale.In this paper, we develop and evaluate a scale-free, fast, and accurate model for estimating data center network tail latency performance for a given workload, topology, and network configuration. First, we show that path-level simulations---simulations of traffic that intersects a given path---produce almost the same aggregate statistics as full network-wide packet-level simulations. We use a simple and fast flow-level fluid simulation in a novel way to capture and summarize essential elements of the path workload, including the effect of cross-traffic on flows on that path. We use this coarse simulation as input to a machine-learning model to predict path-level behavior, and run it on a sample of paths to produce accurate network-wide estimates. Our model generalizes over the choice of congestion control (CC) protocol, CC protocol parameters, and routing. Relative to Parsimon, a state-of-the-art system for rapidly estimating aggregate network tail latency, our approach is significantly faster (5.7\texttimes{}), more accurate (45.9\% less error), and more robust.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
pages = {813–827},
numpages = {15},
keywords = {network simulation, data center networks, approximation, machine learning, network modeling},
location = {Sydney, NSW, Australia},
series = {ACM SIGCOMM '24}
}


@techreport{cloud_gaming,
    author = {Cloudbase},
    title = {Cloud Gaming Latency: What It Is and How to Reduce It},
    institution = {Cloudbase},
    year = {2024},
}

@techreport{voip,
    author = {Tyler Webb},
    title = {VoIP Jitter and Latency: Causes and How to Troubleshoot},
    institution = {GetVoIP},
    year = {2023},
}

@article{continual_learning,
author = {Dietm\"{u}ller, Alexander and Jacob, Romain and Vanbever, Laurent},
title = {On Sample Selection for Continual Learning: A Video Streaming Case Study},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0146-4833},
url = {https://doi.org/10.1145/3687234.3687237},
doi = {10.1145/3687234.3687237},
abstract = {Machine learning (ML) is a powerful tool to model the complexity of communication networks. As networks evolve, we cannot only train once and deploy. Retraining models, known as continual learning, is necessary. Yet, to date, there is no established methodology to answer the key questions: With which samples to retrain? When should we retrain?We address these questions with the sample selection system Memento, which maintains a training set with the "most useful" samples to maximize sample space coverage. Memento particularly benefits rare patterns---the notoriously long "tail" in networking---and allows assessing rationally when retraining may help, i.e., when the coverage changes.We deployed Memento on Puffer, the live-TV streaming project, and achieved a 14 \% reduction of stall time, 3.5\texttimes{} the improvement of random sample selection. Memento is model-agnostic and can be applied beyond video streaming.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {10–35},
numpages = {26},
keywords = {continual learning, machine learning, video streaming}
}

@INPROCEEDINGS{xgboost_pred,
  author={Chen, Yige and Zang, Tianning and Zhang, Yongzheng and Zhou, Yuan and Wang, Yipeng},
  booktitle={2019 IEEE 27th International Conference on Network Protocols (ICNP)}, 
  title={Rethinking Encrypted Traffic Classification: A Multi-Attribute Associated Fingerprint Approach}, 
  year={2019},
  volume={},
  number={},
  pages={1-11},
  keywords={Cryptography;Servers;Protocols;Training;Markov processes;Correlation;Encrypted traffic classification;SSL/TLS;domain name;certificate;application data;network management},
  doi={10.1109/ICNP.2019.8888043}}



@misc{diffserv,
    series =    {Request for Comments},
    number =    3260,
    howpublished =  {RFC 3260},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC3260},
    url =       {https://www.rfc-editor.org/info/rfc3260},
    author =    {Daniel B. Grossman},
    title =     {{New Terminology and Clarifications for Diffserv}},
    pagetotal = 10,
    year =      2002,
    month =     apr,
    day =       1,
    abstract =  {This memo captures Diffserv working group agreements concerning new and improved terminology, and provides minor technical clarifications. It is intended to update RFC 2474, RFC 2475 and RFC 2597. When RFCs 2474 and 2597 advance on the standards track, and RFC 2475 is updated, it is intended that the revisions in this memo will be incorporated, and that this memo will be obsoleted by the new RFCs. This memo provides information for the Internet community.},
}

@misc{efphb,
    series =    {Request for Comments},
    number =    3246,
    howpublished =  {RFC 3246},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC3246},
    url =       {https://www.rfc-editor.org/info/rfc3246},
    author =    {Jean-Yves Le Boudec and William Courtney and Jon Bennett and Shahram Davari and Dimitrios Stiliadis and Kent Benson and Victor Firoiu and Dr. Bruce S. Davie and Anna Charny},
    title =     {{An Expedited Forwarding PHB (Per-Hop Behavior)}},
    pagetotal = 16,
    year =      2002,
    month =     mar,
    abstract =  {This document defines a PHB (per-hop behavior) called Expedited Forwarding (EF). The PHB is a basic building block in the Differentiated Services architecture. EF is intended to provide a building block for low delay, low jitter and low loss services by ensuring that the EF aggregate is served at a certain configured rate. This document obsoletes RFC 2598. {[}STANDARDS-TRACK{]}},
}

@misc{afphb,
    series =    {Request for Comments},
    number =    2597,
    howpublished =  {RFC 2597},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC2597},
    url =       {https://www.rfc-editor.org/info/rfc2597},
    author =    {Walter Weiss and Dr. Juha Heinanen and Fred Baker and John T. Wroclawski},
    title =     {{Assured Forwarding PHB Group}},
    pagetotal = 11,
    year =      1999,
    month =     jun,
    abstract =  {This document defines a general use Differentiated Services (DS) Per-Hop-Behavior (PHB) Group called Assured Forwarding (AF). {[}STANDARDS-TRACK{]}},
}

@Inproceedings{tcpprague,
  Title                    = {{DUALPI2 - Low Latency, Low Loss and Scalable (L4S) AQM}{}},
  Author                   = {Albisser, Olga and De Schepper, Koen and Briscoe, Bob and Tilmans, Olivier and Steen, Henrik},
  Booktitle                = {Proc. Netdev 0x13},
  Year                     = {2019},
  Month                    = mar,

  Abstract-url             = {\url{https://www.netdevconf.org/0x13/session.html?talk-DUALPI2-AQM}},
  Affiliation              = {1,5: Simula Research Lab, 2,4: Nokia Bell-Labs; 3: Independent; 1: Simula Research Labs; 4,7: Uni Oslo},
  Keywords                 = {Data Communication, Networks, Internet, Performance, Latency, Congestion Control, Congestion Avoidance, QoS, Incremental Deployment, Coexistence, Algorithms, Implementation, Design, AQM, Scaling,},
  Owner                    = {RJB},
  Timestamp                = {2019.02.19},
  Url                      = {https://www.files.netdevconf.org/f/febbe8c6a05b4ceab641/?dl=1}
}

@article{ndn_aqm,
title = {End-to-end active queue management with Named-Data Networking},
journal = {Journal of Network and Computer Applications},
volume = {221},
pages = {103772},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103772},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523001911},
author = {Miguel Rodríguez-Pérez and Sergio Herrería-Alonso and J. {Carlos López-Ardao} and Raúl F. Rodríguez-Rubio},
keywords = {Information-centric networking, Named-data networking, Active queue management, Congestion control, CoDel},
abstract = {The innovative information-based Named-Data Networking (NDN) architecture provides a good opportunity to rethink many of the design decisions that are taken for granted in the Internet today. For example, active queue management (AQM) tasks have been traditionally implemented in the routers to alleviate network congestion before their buffers fill up. However, AQM operations could be performed on an end-to-end basis by taking advantage of NDN features. In this paper, we provide an implementation of an AQM algorithm for the NDN architecture that we use to drive a classical AIMD-based congestion control protocol at the receivers. To accomplish this, we take advantage of the 64-bit Congestion Mark field present in the link layer of NDN packets to encode both rate and delay information about each transmission queue along a network path. In order to make the solution scalable, this information is delivered stochastically, guaranteeing that receivers get accurate and updated information about every pertinent queue. This information is enough to implement the well-known controlled delay (CoDel) AQM algorithm. Simulation results show that our client-located CoDel implementation is able to react to congestion when the bottleneck queuing delay surpasses the 5ms target set by the usual in-network CoDel implementation and, at the same time, get a fair and efficient share of the available transmission capacity.}
}

@article{e2e_queue,
author = {Carlucci, Gaetano and De Cicco, Luca and Mascolo, Saverio},
title = {Controlling queuing delays for real-time communication: the interplay of E2E and AQM algorithms},
year = {2018},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {3},
issn = {0146-4833},
url = {https://doi.org/10.1145/3243157.3243158},
doi = {10.1145/3243157.3243158},
abstract = {Real-time media communication requires not only congestion control, but also minimization of queuing delays to provide interactivity. In this work we consider the case of real-time communication between web browsers (WebRTC) and we focus on the interplay of an end-to-end delay-based congestion control algorithm, i.e. the Google congestion control (GCC), with two delay-based AQM algorithms, namely CoDel and PIE, and two flow queuing schedulers, i.e. SFQ and Fq_Codel. Experimental investigations show that, when only GCC flows are considered, the end-to-end algorithm is able to contain queuing delays without AQMs. Moreover the interplay of GCC flows with PIE or CoDel leads to higher packet losses with respect to the case of a DropTail queue. In the presence of concurrent TCP traffic, PIE and CoDel reduce the queuing delays with respect to DropTail at the cost of increased packet losses. In this scenario flow queuing schedulers offer a better solution.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {jul},
articleno = {1},
numpages = {7},
keywords = {WebRTC, active queue management, congestion control}
}

@article{ts_queue_mgmt,
title = {Queue assignment for fixed-priority real-time flows in time-sensitive networks: Hardness and algorithm},
journal = {Journal of Systems Architecture},
volume = {116},
pages = {102141},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102141},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121001041},
author = {Yuhan Lin and Xi Jin and Tianyu Zhang and Meiling Han and Nan Guan and Qingxu Deng},
keywords = {Resource management, Industrial internet of things, Real-time scheduling, Time-sensitive networks},
abstract = {Time sensitive networks (TSNs) enable deterministic real-time communication over Ethernet networks. According to IEEE 802.1Qbv standards, TSN switches use gates between queues and their corresponding egress ports to facilitate timing-deterministic communications. Management of switch resources, such as queues, has a significant impact on the schedulability of real-time flows. In this paper, we look into the theoretical foundation of queue management in TSN switches. We prove that the queue assignment problem for real-time flows on time sensitive networks under static priority scheduling is NP-hard in the strong sense, even if the number of queues per port is 3. Then we formulate the problem as a satisfiability modulo theories (SMT) specification. Besides, we propose a worst case response time analysis and a fast heuristic algorithms by eliminating scheduling conflicts. Experiments with randomly generated workload demonstrate the effectiveness of our algorithms for queue assignment of real-time flows.}
}

@inproceedings{10.1145/2619239.2626309,
author = {Perry, Jonathan and Ousterhout, Amy and Balakrishnan, Hari and Shah, Devavrat and Fugal, Hans},
title = {Fastpass: a centralized "zero-queue" datacenter network},
year = {2014},
isbn = {9781450328364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2619239.2626309},
doi = {10.1145/2619239.2626309},
abstract = {An ideal datacenter network should provide several properties, including low median and tail latency, high utilization (throughput), fair allocation of network resources between users or applications, deadline-aware scheduling, and congestion (loss) avoidance. Current datacenter networks inherit the principles that went into the design of the Internet, where packet transmission and path selection decisions are distributed among the endpoints and routers. Instead, we propose that each sender should delegate control---to a centralized arbiter---of when each packet should be transmitted and what path it should follow.This paper describes Fastpass, a datacenter network architecture built using this principle. Fastpass incorporates two fast algorithms: the first determines the time at which each packet should be transmitted, while the second determines the path to use for that packet. In addition, Fastpass uses an efficient protocol between the endpoints and the arbiter and an arbiter replication strategy for fault-tolerant failover. We deployed and evaluated Fastpass in a portion of Facebook's datacenter network. Our results show that Fastpass achieves high throughput comparable to current networks at a 240x reduction is queue lengths (4.35 Mbytes reducing to 18 Kbytes), achieves much fairer and consistent flow throughputs than the baseline TCP (5200x reduction in the standard deviation of per-flow throughput with five concurrent connections), scalability from 1 to 8 cores in the arbiter implementation with the ability to schedule 2.21 Terabits/s of traffic in software on eight cores, and a 2.5x reduction in the number of TCP retransmissions in a latency-sensitive service at Facebook.},
booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
pages = {307–318},
numpages = {12},
keywords = {arbiter, centralized, data plane, datacenter, high throughput, low latency, scheduling, zero-queue},
location = {Chicago, Illinois, USA},
series = {SIGCOMM '14}
}


@article{fastpass,
author = {Perry, Jonathan and Ousterhout, Amy and Balakrishnan, Hari and Shah, Devavrat and Fugal, Hans},
title = {Fastpass: a centralized "zero-queue" datacenter network},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2626309},
doi = {10.1145/2740070.2626309},
abstract = {An ideal datacenter network should provide several properties, including low median and tail latency, high utilization (throughput), fair allocation of network resources between users or applications, deadline-aware scheduling, and congestion (loss) avoidance. Current datacenter networks inherit the principles that went into the design of the Internet, where packet transmission and path selection decisions are distributed among the endpoints and routers. Instead, we propose that each sender should delegate control---to a centralized arbiter---of when each packet should be transmitted and what path it should follow.This paper describes Fastpass, a datacenter network architecture built using this principle. Fastpass incorporates two fast algorithms: the first determines the time at which each packet should be transmitted, while the second determines the path to use for that packet. In addition, Fastpass uses an efficient protocol between the endpoints and the arbiter and an arbiter replication strategy for fault-tolerant failover. We deployed and evaluated Fastpass in a portion of Facebook's datacenter network. Our results show that Fastpass achieves high throughput comparable to current networks at a 240x reduction is queue lengths (4.35 Mbytes reducing to 18 Kbytes), achieves much fairer and consistent flow throughputs than the baseline TCP (5200x reduction in the standard deviation of per-flow throughput with five concurrent connections), scalability from 1 to 8 cores in the arbiter implementation with the ability to schedule 2.21 Terabits/s of traffic in software on eight cores, and a 2.5x reduction in the number of TCP retransmissions in a latency-sensitive service at Facebook.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {307–318},
numpages = {12},
keywords = {arbiter, centralized, data plane, datacenter, high throughput, low latency, scheduling, zero-queue}
}

@article{tcp_cubic,
author = {Ha, Sangtae and Rhee, Injong and Xu, Lisong},
title = {CUBIC: a new TCP-friendly high-speed TCP variant},
year = {2008},
issue_date = {July 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1400097.1400105},
doi = {10.1145/1400097.1400105},
abstract = {CUBIC is a congestion control protocol for TCP (transmission control protocol) and the current default TCP algorithm in Linux. The protocol modifies the linear window growth function of existing TCP standards to be a cubic function in order to improve the scalability of TCP over fast and long distance networks. It also achieves more equitable bandwidth allocations among flows with different RTTs (round trip times) by making the window growth to be independent of RTT -- thus those flows grow their congestion window at the same rate. During steady state, CUBIC increases the window size aggressively when the window is far from the saturation point, and the slowly when it is close to the saturation point. This feature allows CUBIC to be very scalable when the bandwidth and delay product of the network is large, and at the same time, be highly stable and also fair to standard TCP flows. The implementation of CUBIC in Linux has gone through several upgrades. This paper documents its design, implementation, performance and evolution as the default TCP algorithm of Linux.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {64–74},
numpages = {11}
}

@article{tcp_bbr,
author = {Cardwell, Neal and Cheng, Yuchung and Gunn, C. Stephen and Yeganeh, Soheil Hassas and Van Jacobson},
title = {BBR: congestion-based congestion control},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3009824},
doi = {10.1145/3009824},
abstract = {Measuring bottleneck bandwidth and round-trip propagation time.},
journal = {Commun. ACM},
month = {jan},
pages = {58–66},
numpages = {9}
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703}, 
}

@software{Falcon_PyTorch_Lightning_2019,
author = {Falcon, William and {The PyTorch Lightning team}},
doi = {10.5281/zenodo.3828935},
license = {Apache-2.0},
month = mar,
title = {{PyTorch Lightning}},
url = {https://github.com/Lightning-AI/lightning},
version = {1.4},
year = {2019}
}

@inproceedings{relu,
 author = {Hahnloser, Richard and Seung, H. Sebastian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/c8cbd669cfb2f016574e9d147092b5bb-Paper.pdf},
 volume = {13},
 year = {2000}
}

@inproceedings{harsha_latency,
author = {Madhyastha, Harsha V. and Anderson, Thomas and Krishnamurthy, Arvind and Spring, Neil and Venkataramani, Arun},
title = {A structural approach to latency prediction},
year = {2006},
isbn = {1595935614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1177080.1177092},
doi = {10.1145/1177080.1177092},
abstract = {Several models have been recently proposed for predicting the latency of end to end Internet paths. These models treat the Internet as a black-box, ignoring its internal structure. While these models are simple, they can often fail systematically; for example, the most widely used models use metric embeddings that predict no benefit to detour routes even though half of all Internet routes can benefit from detours.In this paper, we adopt a structural approach that predicts path latency based on measurements of the Internet's routing topology, PoP connectivity, and routing policy. We find that our approach outperforms Vivaldi, the most widely used black-box model. Furthermore, unlike metric embeddings, our approach successfully predicts 65\% of detour routes in the Internet. The number of measurements used in our approach is comparable with that required by black box techniques, but using traceroutes instead of pings.},
booktitle = {Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement},
pages = {99–104},
numpages = {6},
keywords = {route measurements, latency prediction, internet topology},
location = {Rio de Janeriro, Brazil},
series = {IMC '06}
}

@inproceedings{matthew_latency,
author = {Luckie, Matthew and Hyun, Young and Huffaker, Bradley},
title = {Traceroute probe method and forward IP path inference},
year = {2008},
isbn = {9781605583341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1452520.1452557},
doi = {10.1145/1452520.1452557},
abstract = {Several traceroute probe methods exist, each designed to perform better in a scenario where another fails. This paper examines the effects that the choice of probe method has on the inferred forward IP path by comparing the paths inferred with UDP, ICMP, and TCP-based traceroute methods to (1) a list of routable IP addresses, (2) a list of known routers, and (3) a list of well-known websites. We further compare methods by examining seven months of macroscopic Internet topology data collected by CAIDA's Archipelago infrastructure.We found significant differences in the topology observed using different probe methods. In particular, we found that ICMP-based traceroute methods tend to successfully reach more destinations, as well as collect evidence of a greater number of AS links. UDP-based methods infer the greatest number of IP links, despite reaching the fewest destinations. We hypothesise that some per-flow load balancers implement different forwarding policies for TCP and UDP, and run a specific experiment to confirm this hypothesis.},
booktitle = {Proceedings of the 8th ACM SIGCOMM Conference on Internet Measurement},
pages = {311–324},
numpages = {14},
keywords = {macroscopic internet topology discovery, traceroute},
location = {Vouliagmeni, Greece},
series = {IMC '08}
}

@inproceedings{kevin_latency,
author = {Vermeulen, Kevin and Gurmericliler, Ege and Cunha, Italo and Choffnes, David and Katz-Bassett, Ethan},
title = {Internet scale reverse traceroute},
year = {2022},
isbn = {9781450392594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517745.3561422},
doi = {10.1145/3517745.3561422},
abstract = {Knowledge of Internet paths allows operators and researchers to better understand the Internet and troubleshoot problems. Paths are often asymmetric, so measuring just the forward path only gives partial visibility. Despite the existence of Reverse Traceroute, a technique that captures reverse paths (the sequence of routers traversed by traffic from an arbitrary, uncontrolled destination to a given source), this technique did not fulfill the needs of operators and the research community, as it had limited coverage, low throughput, and inconsistent accuracy. In this paper we design, implement and evaluate revtr 2.0, an Internet-scale Reverse Traceroute system that combines novel measurement approaches and studies with a large-scale deployment to improve throughput, accuracy, and coverage, enabling the first exploration of reverse paths at Internet scale. revtr 2.0 can run 15M reverse traceroutes in one day. This scale allows us to open the system to external sources and users, and supports tasks such as traffic engineering and troubleshooting.},
booktitle = {Proceedings of the 22nd ACM Internet Measurement Conference},
pages = {694–715},
numpages = {22},
keywords = {reverse traceroute, internet scale, internet measurements},
location = {Nice, France},
series = {IMC '22}
}

@inproceedings{xgboost,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@misc{netgpt,
      title={NetGPT: Generative Pretrained Transformer for Network Traffic}, 
      author={Xuying Meng and Chungang Lin and Yequan Wang and Yujun Zhang},
      year={2023},
      eprint={2304.09513},
      archivePrefix={arXiv},
      primaryClass={cs.NI},
      url={https://arxiv.org/abs/2304.09513}, 
}

@misc{trafficgpt,
      title={TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models}, 
      author={Siyao Zhang and Daocheng Fu and Zhao Zhang and Bin Yu and Pinlong Cai},
      year={2023},
      eprint={2309.06719},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2309.06719}, 
}

@misc{memorizingtransformers,
      title={Memorizing Transformers}, 
      author={Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and Christian Szegedy},
      year={2022},
      eprint={2203.08913},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.08913}, 
}














