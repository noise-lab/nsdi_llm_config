%!TEX root = main.tex
%!TEX spellcheck = en_US

\begin{abstract}
Misconfiguration detection in router configurations is crucial for maintaining network stability, security, and performance.
Traditional methods like formal model checkers and consistency checkers either rely on manual rule definitions, requiring heavy domain expertise, or assume deviations from standard settings always indicate errors, overlooking context-specific configurations. 
LLM-based Q\&A models have emerged as a solution by leveraging transformer-based architectures pre-trained on vast datasets, including networking documents. These models enhance misconfiguration detection by capturing both syntax and semantic relationships, benefiting from pre-learned context on how configurations should function.
However, detecting misconfigurations often requires task-specific context from the actual configuration files for accurate inference. Current methods such as partition-based prompting fail to capture this, missing crucial context and interdependencies spread across the files, resulting in incomplete or inaccurate detection.

% LLM-based Q&A models have emerged as a solution by leveraging transformer architectures and pre-training on vast datasets including networking-related documents, making them adept at capturing common syntax and semantic misconfigruation from general understand of network configruation context. 
% The limitations have led to the adoption of LLM-based Q\&A models, which leverage transformer-based architectures to process and interpret both the syntax and semantics of configurations. Transformers, with their self-attention mechanisms, excel at capturing long-range dependencies and contextual relationships within sequences of text by pre-training on vast datasets, including networking-related documents.
% Yet, the prevalent use of partition-based prompting in LLMs—where configuration files are divided into isolated sections for analysis—fails to account for the interdependencies and contextual relationships that are spread across the configuration file, leading to incomplete or inaccurate misconfiguration detection.

We introduce Context-Aware Iterative Prompting (\sysname{}), a framework that automates task-specific context extraction and optimizes LLM prompts for more precise router misconfiguration detection. \sysname{} addresses three core challenges: (1) efficiently mining relevant context from complex, hierarchical configuration files, (2) accurately distinguishing between pre-defined and user-defined parameter values to prevent irrelevant context from being introduced, and (3) managing context overload by employing iterative, guided interactions with the model. Our evaluations, using synthetic, real-world, and large-scale campus network configurations, show that \sysname{} significantly improves the accuracy of LLM-based misconfiguration detection, outperforming or matching existing methods across all tested scenarios.
\end{abstract}