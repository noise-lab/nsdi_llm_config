%!TEX root = main.tex
%!TEX spellcheck = en_US

\begin{abstract}
Misconfiguration detection in router configurations is crucial for maintaining network stability, security, and performance. Misconfigurations, whether in existing setups or introduced through updates, can lead to security vulnerabilities, inefficient routing, or even network outages. Traditional methods for detecting these issues include formal model checkers and data mining tools. However, these approaches have notable shortcomings: model checkers rely heavily on manual rule definitions, which require deep domain expertise, while data mining tools operate under the assumption that deviations from a standard or universal setting always indicate errors, which overlooks the context-specific nature of certain configurations.
% The limitations of these methods have led to the adoption of LLM-based Q\&A models, which offer the potential to understand \aaron{Does LLM actually understand? provide ML explanations/interpretations} both the syntax and semantics of configurations.
The limitations of these methods have led to the adoption of LLM-based Q\&A models, which leverage transformer-based architectures to process and interpret both the syntax and semantics of configurations. Transformers, with their self-attention mechanisms, excel at capturing long-range dependencies and contextual relationships within sequences of text by pre-training on vast datasets, including networking-related documents.
Yet, current LLM approaches face significant challenges. The prevalent use of partition-based prompting—where configuration files are divided into isolated sections for analysis—fails to account for the interdependencies and contextual relationships that are spread across the configuration file, leading to incomplete or inaccurate misconfiguration detection.

We introduce Context-Aware Iterative Prompting (\sysname{}), a framework that automates context extraction and optimizes LLM prompts for more precise router misconfiguration detection. \sysname{} addresses three core challenges: (1) efficiently mining relevant context from complex, hierarchical configuration files, (2) accurately distinguishing between pre-defined and user-defined parameter values to prevent irrelevant context from being introduced, and (3) managing context overload by employing iterative, guided interactions with the model. Our evaluations, using synthetic, real-world, and large-scale campus network configurations, show that \sysname{} significantly improves the accuracy of LLM-based misconfiguration detection, outperforming or matching existing methods across all tested scenarios.
\chase{Maybe Aaron you can help making this a bit more concise?}
\end{abstract}