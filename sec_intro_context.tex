%!TEX root = main.tex
%!TEX spellcheck = en_US

\section{Introduction}
\label{sec:intro}

Misconfiguration detection in router configurations is crucial for maintaining the stability, security, and performance of network infrastructures. Whether due to overlooked errors in existing setups or mistakes introduced during configuration changes, misconfigurations can lead to black holes, unintended access routes, or inefficient routing paths. %These disruptions not only compromise the network’s integrity but also result in costly downtimes or security breaches. 
For instance, a misconfigured access control list (ACL) could allow unauthorized traffic, exposing the network to potential attacks, while a poorly defined routing policy might create traffic loops, making services inaccessible and degrading network performance.

Researchers and practitioners have developed a plethora of tools for detecting network misconfigurations. Model checkers~\cite{fogel2015general, beckett2017general, abhashkumar2020tiramisu, prabhu2020plankton} model a network's routing and forwarding behaviors based on protocol semantics and device configurations, and check whether engineer-specified reachability and resilience policies are satisfied. Consistency checkers~\cite{kakarla2024diffy, kakarla2020finding, le2006minerals, feamster2005detecting} compare configurations within and across devices and flag deviations from best practices and inconsistencies. LLM-based tools~\cite{bogdanov2024leveraging,chen2024automatic,wang2024identifying,liu2024large, wang2024netconfeval, lian2023configuration} use...\aaron{How to summarize what LLM-based tools do?} \aaron{To categorize: \cite{al2009network, ritchey2000using,al2011configchecker, jeffrey2009model, le2008detecting,le2006characterization}}

A fundamental feature of all configurations checkers is their reliance on {\em context}. Model checkers require protocol specifications, current (or proposed) device configurations, and forwarding policies; consistency checkers require configurations from multiple devices and a set of best practices; and LLMs require...\aaron{???}. Moreover, configuration checkers' accuracy depends on the correctness and completeness of the provided context. For example, providing an incomplete set of forwarding policies, best practices, or \aaron{incomplete example for LLMs} may cause misconfigurations to be missed. Similarly, improperly modeling protocol semantics~\cite{birkner2021metha}, excluding certain configurations, or \aaron{incorrect example for LLMs} may cause portions of configurations to be falsely labeled as incorrect.

While some context is easy to provide--e.g., current (or proposed) configurations--many forms of context require significant manual effort. For example, a detailed understanding of routing protocols, their interactions~\cite{le2007rr}, and vendor-specific nuances~\cite{birkner2021metha} is required to accurately model protocol semantics. Similarly, forwarding policies are rarely explicitly recorded and only certain types of policies can be reverse-engineered from configurations~\cite{birkner2020config2spec, kheradmand2020anime}--which may not even be correct!

LLM-based approaches benefit from pre-learned context, but some context is task specific. \aaron{Need to elaborate}

In this paper, we introduce a new framework, Context-Aware Iterative Prompting (\sysname{}), designed to automate context extraction and optimize prompting to LLMs for accurate router misconfiguration detection. We identify three key challenges in realizing this framework and provide solutions to address them:

\mypara{Challenge 1 - Efficient and Accurate Context Mining}
Network configuration files are often lengthy and complex, with interrelated lines that require careful context extraction. The challenge lies in efficiently identifying and extracting relevant context to reduce computational costs and avoid introducing irrelevant information that could impair the accuracy of misconfiguration detection. \textbf{Solution}: We address this by leveraging the hierarchical structure of configuration files, modeling them as trees where each line is a unique path from root to parameter value. This allows us to systematically mine three types of context—neighboring, similar, and referenceable configurations—ensuring that the LLM receives the most relevant information for accurate analysis.

\mypara{Challenge 2 - Parameter Values Ambiguity in Referenceable Context}
Configurations contain both pre-defined and user-defined parameter values, with the latter requiring context for proper interpretation. When extracting referenceable context, misidentifying these can lead to irrelevant context mining and increased computational overhead. \textbf{Solution}: To accurately differentiate these values, we implement an existence-based method within the configuration tree, identifying user-defined values by their presence as intermediate nodes in other paths. Additionally, we use a majority-voting approach to resolve ambiguities, ensuring consistency in how values are treated across different contexts.

\mypara{Challenge 3 - Managing Context Overload in Prompts}
Even with precise context extraction, the volume of relevant information can be extensive, risking prompt overload when fed into LLMs. Excessive context can dilute relevance, reduce coherence, and degrade the model’s performance, necessitating strategies to manage and streamline the input effectively. \textbf{Solution}: Our framework mitigates this by allowing the LLM to iteratively request specific types of context, refining the prompt based on the model’s feedback. This approach prevents overload and enhances the model’s ability to focus on the most relevant information, leading to more precise misconfiguration detection.

We evaluate our framework through two distinct case studies: (1) synthetic change verification: using configuration snapshots from real networks, we introduce synthetic modifications to simulate potential changes that network operators may make (both correct and incorrect). We demonstrate that the model accurately infers and detects these changes. (2) comprehensive real configuration snapshot verification: we exhaustively run \sysname{} on configuration snapshots from a medium-sized campus network to detect overlooked misconfigurations in real-world environments. Our evaluation shows that \sysname{} consistently outperforms or matches the accuracy of existing methods across all scenarios.