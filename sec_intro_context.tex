%!TEX root = main.tex
%!TEX spellcheck = en_US

\section{Introduction}
\label{sec:intro}

Misconfiguration detection in router configurations is crucial for maintaining the stability, security, and performance of network infrastructures. Whether due to overlooked errors in existing setups or mistakes introduced during configuration changes, misconfigurations can lead to black holes, unintended access routes, inefficient routes, or other issues. %These disruptions not only compromise the network’s integrity but also result in costly downtimes or security breaches. 
For example, a misconfigured access control list (ACL) may allow unauthorized traffic, exposing the network to potential attacks, while a bad routing policy may create loops, rendering services inaccessible and degrading performance.

Researchers and practitioners have developed a plethora of tools for detecting network misconfigurations. Model checkers~\cite{fogel2015general, beckett2017general, abhashkumar2020tiramisu, prabhu2020plankton, zhang2022sre, steffen2020netdice, ye2020hoyan, ritchey2000using,al2011configchecker, jeffrey2009model} model a network's routing and forwarding behaviors based on protocol semantics and device configurations, and check whether engineer-specified reachability and resilience policies are satisfied. Consistency checkers~\cite{kakarla2024diffy, kakarla2020finding, le2006minerals, feamster2005detecting, tang2021campion,le2008detecting,le2006characterization} compare configurations within and across devices and flag inconsistencies and deviations from best practices. LLM-based Q\&A tools~\cite{bogdanov2024leveraging,chen2024automatic,wang2024identifying,liu2024large, wang2024netconfeval, lian2023configuration} parse configuration files and query pre-trained sequential transformer models to detect syntax and subtle semantic issues.

A fundamental feature of all configuration checkers is their reliance on {\em context}. Model checkers require protocol specifications, current (or proposed) device configurations, and forwarding policies; consistency checkers require configurations from multiple devices and a set of best practices; and LLMs require a sufficient number of relevant configuration lines in query prompts. Moreover, the correctness and completeness of this context impacts configuration checkers' accuracy. For example, providing an incomplete set of forwarding policies, best practices, or configuration lines
%too few lines of configurations in LLM prompts 
may cause misconfigurations to be missed. Similarly, improperly modeling protocol semantics~\cite{birkner2021metha, ye2020hoyan}, excluding certain configurations~\cite{xu2023netcov}, or supplying unrelated configuration lines~\cite{liskavets2024prompt,tian2024examining,khurana2024and, shvartzshnaider2024llm} may cause false alarms.

While some context is easy to provide---\eg, current config\-urations---many forms of context require significant manual effort. For example, a detailed understanding of routing protocols, their interactions~\cite{le2007rr}, and vendor-specific nuances~\cite{ye2020hoyan} is required to accurately model networks' routing and forwarding behaviors. Similarly, awareness of packet and route filter semantics is required to accurately detect inconsistencies in filter configurations~\cite{kakarla2020finding}.
%As another example, forwarding policies are rarely recorded and only certain types of policies can be reverse-engineered from configurations~\cite{birkner2020config2spec, kheradmand2020anime}. 

Since LLMs are trained on vast volumes of data---which likely includes protocol standards, vendor documentation, and sample configurations---a significant amount of \textit{generic configuration context} is already embedded in large-scale pre-trained models (PTMs)~\cite{qiu2020pre}. However, such pre-learned generic context often needs to be supplemented with \textit{network-specific context}, which details the actual configurations within the current configuration file under analysis. Including this network-specific context in the prompt is essential for detecting misconfigurations unique to the device or network.
Current approaches like feeding the entire file into a single prompt~\cite{lican,li2024long} can result in context and token overload, diluting the model's focus. Conversely, partitioning the file into multiple prompts~\cite{lian2023configuration,chen2024automatic,wang2024identifying} ignores dependencies between configuration lines, causing potential errors to be missed. Therefore, network-specific context must be carefully extracted and structured to ensure proper focus and coverage.

In this paper, we introduce a new framework, Context-Aware Iterative Prompting (\sysname{}), to automate context extraction and optimize prompting to LLMs for accurate router misconfiguration detection. We identify three key challenges in realizing this framework and provide solutions for them:

\mypara{Challenge 1 - Automatic and Accurate Context Mining}
Network configurations are often lengthy and complex~\cite{benson2009complexitymetrics}, with interrelated lines that require careful context extraction. The challenge lies in automatically identifying and extracting relevant context to reduce computational costs and avoid introducing irrelevant information that could impair the accuracy of misconfiguration detection. \textbf{Solution}: We address this by leveraging the hierarchical structure of configuration files, modeling them as trees where each line is a unique path from root to parameter value. This allows us to systematically mine three types of context---neighboring, similar, and referenced configuration statements---ensuring the LLM receives the most relevant information for accurate analysis.

\mypara{Challenge 2 - Parameter Values Ambiguity in Referenceable Context}
Configurations contain both pre-defined and user-defined parameter values, with the latter requiring context for proper interpretation. For example, an interface might reference a user-defined Access Control List (ACL), which must be interpreted based on related configuration lines. When extracting referenced context, misidentifying these can lead to irrelevant context mining and increased computational overhead. \textbf{Solution}: To accurately differentiate these values, we apply an existence-based method within the configuration tree, identifying user-defined values by their presence as intermediate nodes in other paths. Additionally, we use majority-voting to resolve ambiguities, ensuring consistency in how values are treated across different contexts.

\mypara{Challenge 3 - Managing Context Overload in Prompts}
Even with precise context extraction, the volume of relevant information can be extensive. 
%, risking prompt overload when fed into LLMs. Excessive context 
Feeding excessive context into LLMs can dilute relevance, reduce coherence, and degrade the model’s performance.
%, necessitating strategies to manage and streamline the input effectively. 
\textbf{Solution}: Our framework mitigates this by allowing the LLM to iteratively request specific types of context, refining the prompt based on the model’s feedback. This approach prevents overload and enhances the model’s ability to focus on the most relevant information, leading to more precise misconfiguration detection.

We evaluate our framework through two distinct case studies.
%: (1) synthetic change verification: 
First, using configuration snapshots from real networks, we introduce {\em synthetic} modifications to simulate potential changes that network operators may make (both correct and incorrect). We demonstrate that the model accurately infers and detects these changes. 
%(2) comprehensive real configuration snapshot verification: 
Second, we exhaustively run \sysname{} on configuration snapshots from a medium-sized campus network to detect overlooked misconfigurations in {\em real-world} environments. Our evaluation shows that \sysname{} consistently outperforms or matches the accuracy of existing methods across all scenarios.