%!TEX root = main.tex
%!TEX spellcheck = en_US

\section{Introduction}
\label{sec:intro}

Misconfiguration detection in router configurations is crucial for maintaining the stability, security, and performance of network infrastructures. Whether due to overlooked errors in existing setups or mistakes introduced during configuration changes, misconfigurations can lead to black holes, unintended access routes, or inefficient routing paths. %These disruptions not only compromise the network’s integrity but also result in costly downtimes or security breaches. 
For instance, a misconfigured access control list (ACL) may allow unauthorized traffic, exposing the network to potential attacks, while a poorly defined routing policy may create loops, making services inaccessible and degrading performance.

Researchers and practitioners have developed a plethora of tools for detecting network misconfigurations. Model checkers~\cite{fogel2015general, beckett2017general, abhashkumar2020tiramisu, prabhu2020plankton, zhang2022sre, steffen2020netdice, ye2020hoyan, ritchey2000using,al2011configchecker, jeffrey2009model} model a network's routing and forwarding behaviors based on protocol semantics and device configurations, and check whether engineer-specified reachability and resilience policies are satisfied. Consistency checkers~\cite{kakarla2024diffy, kakarla2020finding, le2006minerals, feamster2005detecting, tang2021campion,le2008detecting,le2006characterization} compare configurations within and across devices and flag inconsistencies and deviations from best practices. LLM-based Q\&A tools~\cite{bogdanov2024leveraging,chen2024automatic,wang2024identifying,liu2024large, wang2024netconfeval, lian2023configuration} parse configuration files and query pre-trained sequential transformer models for detecting both syntactical and subtle semantic issues.

A fundamental feature of all configurations checkers is their reliance on {\em context}. Model checkers require protocol specifications, current (or proposed) device configurations, and forwarding policies; consistency checkers require configurations from multiple devices and a set of best practices; and LLMs rely on incorporating a sufficient number of relevant configuration lines in query prompts. Moreover, the correctness and completeness of this context impacts configuration checkers' accuracy. For example, providing an incomplete set of forwarding policies, best practices, or too few lines of configurations in LLM prompts may cause misconfigurations to be missed. Similarly, improperly modeling protocol semantics~\cite{birkner2021metha, ye2020hoyan}, excluding certain configurations~\cite{xu2023netcov}, or supplying unrelated configuration lines in LLM prompts~\cite{liskavets2024prompt,tian2024examining,khurana2024and, shvartzshnaider2024llm} may cause false alarms.

While some context is easy to provide---\eg, current config\-urations---many forms of context require significant manual effort. For example, a detailed understanding of routing protocols, their interactions~\cite{le2007rr}, and vendor-specific nuances~\cite{ye2020hoyan} is required to accurately model networks' routing and forwarding behaviors. Similarly, awareness of packet and route filter semantics is required to accurately detect inconsistencies in filter configurations~\cite{kakarla2020finding}.
%As another example, forwarding policies are rarely recorded and only certain types of policies can be reverse-engineered from configurations~\cite{birkner2020config2spec, kheradmand2020anime}. 

Since LLMs are trained on vast volumes of data---which likely includes protocol standards, vendor documentation, and sample configurations---a significant amount of context is already embedded in large-scale pre-trained models (PTMs)~\cite{qiu2020pre}. However, such pre-learned context often need to be supplemented with task-specific context, which details the specific configurations and states within the current configuration file under analysis. Including this task-specific context in the prompt is essential for detecting misconfigurations unique to the file or device.
Current approaches like feeding the entire file into a single prompt can result in context and token overload, diluting the model's focus. Conversely, partitioning the file into multiple prompts disrupts interdependencies between configuration lines, causing incomplete context and potential errors to be missed. Therefore, task-specific context must be carefully extracted and structured to ensure proper focus and coverage.

In this paper, we introduce a new framework, Context-Aware Iterative Prompting (\sysname{}), designed to automate context extraction and optimize prompting to LLMs for accurate router misconfiguration detection. We identify three key challenges in realizing this framework and provide solutions to address them:

\mypara{Challenge 1 - Efficient and Accurate Context Mining}
Network configuration files are often lengthy and complex, with interrelated lines that require careful context extraction. The challenge lies in efficiently identifying and extracting relevant context to reduce computational costs and avoid introducing irrelevant information that could impair the accuracy of misconfiguration detection. \textbf{Solution}: We address this by leveraging the hierarchical structure of configuration files, modeling them as trees where each line is a unique path from root to parameter value. This allows us to systematically mine three types of context—neighboring, similar, and referenceable configurations—ensuring that the LLM receives the most relevant information for accurate analysis.

\mypara{Challenge 2 - Parameter Values Ambiguity in Referenceable Context}
Configurations contain both pre-defined and user-defined parameter values, with the latter requiring context for proper interpretation. When extracting referenceable context, misidentifying these can lead to irrelevant context mining and increased computational overhead. \textbf{Solution}: To accurately differentiate these values, we implement an existence-based method within the configuration tree, identifying user-defined values by their presence as intermediate nodes in other paths. Additionally, we use a majority-voting approach to resolve ambiguities, ensuring consistency in how values are treated across different contexts.

\mypara{Challenge 3 - Managing Context Overload in Prompts}
Even with precise context extraction, the volume of relevant information can be extensive, risking prompt overload when fed into LLMs. Excessive context can dilute relevance, reduce coherence, and degrade the model’s performance, necessitating strategies to manage and streamline the input effectively. \textbf{Solution}: Our framework mitigates this by allowing the LLM to iteratively request specific types of context, refining the prompt based on the model’s feedback. This approach prevents overload and enhances the model’s ability to focus on the most relevant information, leading to more precise misconfiguration detection.

We evaluate our framework through two distinct case studies: (1) synthetic change verification: using configuration snapshots from real networks, we introduce synthetic modifications to simulate potential changes that network operators may make (both correct and incorrect). We demonstrate that the model accurately infers and detects these changes. (2) comprehensive real configuration snapshot verification: we exhaustively run \sysname{} on configuration snapshots from a medium-sized campus network to detect overlooked misconfigurations in real-world environments. Our evaluation shows that \sysname{} consistently outperforms or matches the accuracy of existing methods across all scenarios.