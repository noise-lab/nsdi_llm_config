%!TEX root = main.tex
%!TEX spellcheck = en_US

\section{Introduction}
\label{sec:intro}

Detecting router misconfigurations is crucial for the stability, security, and
performance of network infrastructure. Whether due to overlooked errors in
existing setups or mistakes introduced during configuration changes,
misconfigurations can disrupt network behavior, leading to unintended access,
inefficient routing paths, or even network outages. Such disruptions not only
compromise the network’s integrity but may also introduce costly downtime or
security breaches. For instance, a misconfigured access control list (ACL)
could allow unauthorized traffic, exposing the network to potential attacks; a
poorly defined routing policy might create traffic loops, resulting in network
performance, increased congestion, or availability problems.

Current methods for detecting misconfigurations in router configurations fall
into three categories: automated formal model checkers, data mining-based
tools, and Large Language Model (LLM)-based Q\&A models. Model checkers can be
either rule-based, such as Batfish~\cite{fogel2015general} and
rcc~\cite{feamster2005detecting}; or policy-based, such as Batfish,
Minesweeper~\cite{beckett2017general},
Tiramisu~\cite{abhashkumar2020tiramisu}, and most of the other network
configuration verifiers~\cite{kakarla2020finding,prabhu2020plankton,
al2009network, ritchey2000using,al2011configchecker, jeffrey2009model} that
have been developed. These verifiers are designed to validate configurations
by checking their adherence to predefined rules and policies. While are highly
effective, their performance is heavily dependent on the quality of the rules
and policies being inputted, which require substantial domain expertise to
define. Data mining-based tools, like Diffy~\cite{kakarla2024diffy},
Minerals~\cite{le2006minerals}, SelfStarter~\cite{kakarla2020finding}, and
more~\cite{le2008detecting,le2006characterization}, take a different approach
by deriving a universal model through graph-based or data mining techniques,
assuming that any deviation from a standard configuration is a
misconfiguration. While this method works well for maintaining uniformity
across routers, it struggles with context-specific configurations that deviate
from the norm for valid reasons. Finally, there are LLM-based Q\&A
models~\cite{bogdanov2024leveraging,chen2024automatic,wang2024identifying,liu2024large,
wang2024netconfeval}, such as Ciri~\cite{lian2023configuration}, that attempt
to `understand' the syntax and semantic of configurations by treating
misconfiguration detection as a sequential text generation task. These models
leveraging transformer-based architectures~\cite{vaswani2017attention} to
process input configurations as sequences of tokens and are pre-trained using
large-scale datasets, including general text documents and network
configuration texts. This allows the models to learn relationships between
tokens (words or phrases) and their contexts through attention mechanisms, a
key feature of transformer models.


Despite their strengths, each of these methods has significant weaknesses that limit their effectiveness in accurately detecting router misconfigurations. Model checkers are limited by the need for comprehensive and precise rule and policy definitions, which can be challenging in dynamic network environments~\cite{graf1990limits,baier2008principles}.
Data mining-based tools lack the ability to understand the underlying semantics of configurations, making them prone to false positives in cases where deviations are intentional and context-specific. LLM-based Q\&A models for configuration validation, while powerful, face challenges in how they are prompted. The current practice of partition-based prompting, where configuration files are divided into isolated sections for analysis, fails to capture the interdependencies and contextual nuances spread across the configuration. This results in a reduced ability to detect complex misconfigurations that depend on a holistic understanding of the entire configuration file~\cite{liskavets2024prompt,tian2024examining,khurana2024and,shvartzshnaider2024llm}. To address these limitations, there is a need to design a framework that automates the process of context extraction and optimally structures the prompts for LLMs. Such a framework would enhance the accuracy of router misconfiguration detection by ensuring that all relevant context is considered, enabling more precise and reliable verification.

In this paper, we introduce a new framework, Context-Aware Iterative Prompting (\sysname{}), designed to automate context extraction and optimize prompting to LLMs for accurate router misconfiguration detection. We identify three key challenges in realizing this framework and provide solutions to address them:

\mypara{Challenge 1 - Efficient and Accurate Context Mining}
Network configuration files are often lengthy and complex, with interrelated lines that require careful context extraction. The challenge lies in efficiently identifying and extracting relevant context to reduce computational costs and avoid introducing irrelevant information that could impair the accuracy of misconfiguration detection. \textbf{Solution}: We address this by leveraging the hierarchical structure of configuration files, modeling them as trees where each line is a unique path from root to parameter value. This allows us to systematically mine three types of context—neighboring, similar, and referenceable configurations—ensuring that the LLM receives the most relevant information for accurate analysis.

\mypara{Challenge 2 - Parameter Values Ambiguity in Referenceable Context}
Configurations contain both pre-defined and user-defined parameter values, with the latter requiring context for proper interpretation. When extracting referenceable context, misidentifying these can lead to irrelevant context mining and increased computational overhead. \textbf{Solution}: To accurately differentiate these values, we implement an existence-based method within the configuration tree, identifying user-defined values by their presence as intermediate nodes in other paths. Additionally, we use a majority-voting approach to resolve ambiguities, ensuring consistency in how values are treated across different contexts.

\mypara{Challenge 3 - Managing Context Overload in Prompts}
Even with precise context extraction, the volume of relevant information can be extensive, risking prompt overload when fed into LLMs. Excessive context can dilute relevance, reduce coherence, and degrade the model’s performance, necessitating strategies to manage and streamline the input effectively. \textbf{Solution}: Our framework mitigates this by allowing the LLM to iteratively request specific types of context, refining the prompt based on the model’s feedback. This approach prevents overload and enhances the model’s ability to focus on the most relevant information, leading to more precise misconfiguration detection.

We evaluate our framework through two distinct case studies: (1) synthetic change verification: using configuration snapshots from real networks, we introduce synthetic modifications to simulate potential changes that network operators may make (both correct and incorrect). We demonstrate that the model accurately infers and detects these changes. (2) comprehensive real configuration snapshot verification: we exhaustively run \sysname{} on configuration snapshots from a medium-sized campus network to detect overlooked misconfigurations in real-world environments. Our evaluation shows that \sysname{} consistently outperforms or matches the accuracy of existing methods across all scenarios.
