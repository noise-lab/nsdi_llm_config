\section{Background and Motivation}
\label{sec_background}
Network misconfigurations are a common and persistent issue, often leading to significant security vulnerabilities, performance degradation, or even network outages. These misconfigurations can stem from either initial setup errors or subsequent changes made by operators. Given the complexity and interdependency of modern network configurations, manually identifying and rectifying these errors has become impractical, driving the need for automated solutions.

Existing methods for misconfiguration detection in networks fall primarily into two categories: model checking methods and data-mining approaches. Model checkers, such as \textit{Batfish}~\cite{fogel2015general} and \textit{Minesweeper}~\cite{beckett2017general}, analyze the structure and behavior of network configurations by either deriving a logical model of the network or emulating the data-plane that would emerge from the current configuration.
For example, Batfish detects network configuration errors by analyzing both control and data planes before configurations are applied, checking for routing loops, syntax errors, and configuration inconsistencies. While model checkers are popular, they rely heavily on predefined rules and require domain expertise to set up. Moreover, they often struggle with the nuanced, interconnected nature of configuration files, where the meaning and impact of a single line can depend heavily on its broader context.
On the other hand, Minesweeper verifies properties like reachability and load-balancing by converting network configurations into logical formulas.
In contrast to model-checking approaches, data-mining tools such as \textit{Diffy}~\cite{kakarla2024diffy} and \textit{Minerals}~\cite{le2006minerals} take a statistical approach by learning common configuration patterns and detecting deviations as potential errors.
For instance, Diffy learns configuration templates and identifies anomalies by comparing new configurations to learned patterns while Minerals applies association rule mining to detect misconfigurations by learning local policies from router configuration files. However, data-mining approaches tend to oversimplify configurations by treating deviations from standard patterns as misconfigurations, leading to false positives when valid configurations deviate for context-specific reasons.


Given the shortcoming of these existing tools,
people have increasingly turned to Large Language Models (LLMs) for misconfiguration detection~\cite{bogdanov2024leveraging,chen2024automatic,wang2024identifying,liu2024large, wang2024netconfeval} due to their advanced ability to understand and process complex contextual information embedded within network configurations. A state-of-the-art tool in this category is Ciri~\cite{lian2023configuration}, a LLM-based configuration validation framework.

LLMs, specifically transformer-based models~\cite{vaswani2017attention,hill2024transformers,lin2022survey}, excel at capturing intricate relationships between configuration elements by leveraging self-attention mechanisms that dynamically weigh the importance of each token (or configuration parameter) in relation to others, regardless of their distance within the file. This allows LLMs to attend to both local and global dependencies, enabling them to recognize not only syntax and pattern anomalies but also to infer potential misconfigurations based on the underlying semantics of the configuration. The use of position encodings ensures that the order of elements in a configuration file is considered, allowing LLMs to assess the correctness of parameters based on their sequence. Additionally, most LLMs rely on the use of large-scale pre-trained models (PTMs) trained on vast amounts of data~\cite{qiu2020pre}, particularly generative pretrained tranformers (GPTs)~\cite{achiam2023gpt,touvron2023llama,shanahan2024talking,taylor2023galactica,brown2020language,chowdhery2023palm}, enabling them to generalize effectively across various tasks.
These distinct advantages have spurred the use of LLMs across diverse domains~\cite{carion2020end,sheng2019nrtr,neil2020transformers,parmar2018image,chen2021developing,gulati2020conformer}, making their application in network misconfiguration detection a natural progression.

Through this architecture, LLMs are able to generalize across different contexts, even for unseen configurations, enabling them to detect subtle errors that might arise from interactions between different configuration elements—issues often missed by conventional tools. As networks become more complex, with increasingly interdependent components, LLMs' ability to holistically analyze configurations and understand the intent behind them makes them a powerful tool for enhancing the accuracy and reliability of misconfiguration detection.


Despite the powerful capabilities of LLMs, their application in misconfiguration detection still faces a critical challenge: how to effectively prompt these models to ensure they can make accurate detection decisions. Specifically, when examining a particular line in a network configuration file, it’s essential to extract all relevant context—other configuration lines that are related and can potentially aid in misconfiguration detection. Without proper context, the LLM’s ability to detect issues is diminished~\cite{liskavets2024prompt,tian2024examining,khurana2024and, shvartzshnaider2024llm}.

The naive approach would be to feed the entire configuration file to the LLM, either all at once or progressively, and let the model handle the detection. This method, while straightforward, is highly inefficient due to the inherent token length limitations of LLMs~\cite{xue2024repeat,yu2024breaking,gu2023mamba}. More critically, flooding the model with all the configuration data can lead to context overload~\cite{lican,li2024long,qian2024long}, a known issue where the presence of excessive and irrelevant information dilutes the LLM’s capacity to focus on what’s important. This not only reduces the model’s performance but can also result in missing key misconfigurations or generating false positives due to irrelevant context. Network configuration files can be large and complex, with numerous unrelated sections, making it impractical to handle all lines equally without careful context management.

A common solution to this problem has been partition-based prompting, where the configuration file is broken down into smaller chunks or sections, typically based on the order in which the configuration appears in the file. These chunks are then individually fed to the LLM for misconfiguration detection. This approach reduces the token load per prompt and ensures that the model is not overwhelmed by a massive context. However, this method introduces a new set of problems: by treating sections in isolation, it often fails to account for interdependent lines that reside in different parts of the file. Network configurations, unlike typical documents, often contain parameters that interact with or depend on other sections that may not be adjacent in the file.

For example, consider a configuration file where one chunk defines firewall rules and another chunk, further down the file, defines routing policies. A misconfiguration in the firewall rules might depend on how the routing policies are set up, but because partition-based prompting processes these sections independently, this critical context is lost. This is particularly detrimental for detecting dependency-related misconfigurations, where proper detection requires understanding how different sections of the configuration work together. If the partitioned chunk only contains locally adjacent lines that define completely different things, then critical context from other parts of the configuration will be missing, and the model will fail to make accurate inferences.
Another example is where a routing policy is defined in one section of the file and referenced by several other rules throughout the configuration. Partitioning might place the definition and its references into separate chunks, leading the LLM to miss vital connections between them, or worse, misinterpret their relationships.

In this paper, we develop a tool that addresses these specific challenges by introducing a context-aware, iterative prompting mechanism. This mechanism is designed to carefully manage and extract relevant context for each configuration line under review, ensuring that the LLM is provided with sufficient information without overloading it. The following sections detail the challenges we encountered and explain the methodologies we developed to solve them.